---
title: "Mod9-10 (Wk 5)"
author: "Julianna D."
date: "2023-10-05"
output: html_document
---

# Module 9 Intro to Statistical Inference

Review of Standard Errors & CIs
```{r}
n <- 1000
mu <- 3.5
sigma <- 4
v <- rnorm(n, mu, sigma)
s <- sample(v, size = 30, replace = FALSE)
m <- mean(s)
m
```

```{r}
sd <- sd(s)
sd
sem <- sd(s)/sqrt(length(s))
sem
lower <- m - qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci <- c(lower, upper)
ci
```

The Central Limit Theorem
```{r}
lambda <- 14
n <- 10
pop_se <- sqrt(lambda/n)  # the estimated SE
pop_se
```

```{r}
x <- NULL
for (i in 1:1000) {
    x[i] <- mean(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(from = lambda - 4 * sqrt(lambda)/sqrt(n), to = lambda +
    4 * sqrt(lambda)/sqrt(n), length.out = 20), probability = TRUE)
```

```{r}
sd <- sd(x)  # st dev of the sampling distribution
sd
```

```{r}
qqnorm(x)
qqline(x)
```

```{r}
#Sample size of 100
n <- 100
pop_se <- sqrt(lambda/n)  # the estimated SE
pop_se
```
```{r}
x <- NULL
for (i in 1:1000) {
    x[i] <- mean(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(from = lambda - 4 * sqrt(lambda)/sqrt(n), to = lambda +
    4 * sqrt(lambda)/sqrt(n), length.out = 20), probability = TRUE)
```
```{r}
sd <- sd(x)
sd
```
```{r}
qqnorm(x)
qqline(x)
```
We can convert these distributions to standard normals by subtracting off the expected population mean (λ
) and dividing by the standard error of the mean (an estimate of the standard deviation of the sampling distribution) and then plotting a histogram of those values along with a normal curve.
```{r}
curve(dnorm(x, 0, 1), -4, 4, ylim = c(0, 0.8))
z <- (x - lambda)/pop_se
hist(z, breaks = seq(from = -4, to = 4, length.out = 20), probability = TRUE,
    add = TRUE)
```

```{r}
n <- 100
x <- NULL
for (i in 1:1000) {
    x[i] <- sum(rpois(n = n, lambda = lambda))
}
hist(x, breaks = seq(min(x), max(x), length.out = 20), probability = TRUE)
```

## Challenge
```{r}
n <- 1000
x <- 856
phat <- x/n  # our estimate of pi
phat
```

```{r}
n*phat
n*(1-phat)
pop_se <- sqrt((phat) * (1-phat)/n)
pop_se
```
```{r}
curve(dnorm(x, mean = phat, sd = pop_se), phat - 4 * pop_se, phat + 4 * pop_se)
upper <- phat + qnorm(0.975) * pop_se
lower <- phat - qnorm(0.975) * pop_se
ci <- c(lower, upper)
polygon(cbind(c(ci[1], seq(from = ci[1], to = ci[2], length.out = 1000), ci[2]),
    c(0, dnorm(seq(from = ci[1], to = ci[2], length.out = 1000), mean = phat,
        sd = pop_se), 0)), border = "black", col = "gray")
abline(v = ci)
abline(h = 0)
```

```{r}
mu <- 0
sigma <- 1
curve(dnorm(x, mu, 1), mu - 4 * sigma, mu + 4 * sigma, main = "Normal Curve=red\nStudent's t=blue",
    xlab = "x", ylab = "f(x)", col = "red", lwd = 3)
for (i in c(1, 2, 3, 4, 5, 10, 20, 100)) {
    curve(dt(x, df = i), mu - 4 * sigma, mu + 4 * sigma, main = "T Curve", xlab = "x",
        ylab = "f(x)", add = TRUE, col = "blue", lty = 5)
}
```

```{r}
n <- 1e+05
mu <- 3.5
sigma <- 4
x <- rnorm(n, mu, sigma)
sample_size <- 30
s <- sample(x, size = sample_size, replace = FALSE)
m <- mean(s)
m
```

```{r}
sd <- sd(s)
sd
sem <- sd(s)/sqrt(length(s))
sem
lower <- m - qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qnorm(1 - 0.05/2) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_norm <- c(lower, upper)
ci_norm
```

Now, let’s look at the CIs calculated based using the t distribution for the same sample size. For sample size 30, the difference in the CIs is negligible.
```{r}
lower <- m - qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
upper <- m + qt(1 - 0.05/2, df = sample_size - 1) * sem  # (1-alpha)/2 each in the upper and lower tails of the distribution
ci_t <- c(lower, upper)
ci_t
```

Sample=5, CI based on t dist. is much wider
```{r}

```

# Module 10 Classical Hypothesis Testing
